{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment_6"
      ],
      "metadata": {
        "id": "vSp8L9Z5N29Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Atari Breakout: Value Estimation and Policy Optimization"
      ],
      "metadata": {
        "id": "OYQA-SiaOUMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment explores reinforcement learning methods in a simplified version of the classic Atari Breakout game. The task simulates a discrete MDP environment with limited states and actions to demonstrate and compare value estimation algorithms and control strategies. The goal is to evaluate how different approaches behave in estimating value functions and discovering optimal policies."
      ],
      "metadata": {
        "id": "3cegEl6kOXE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Assignment Steps\n",
        "\n",
        "-Define a Simple MDP inspired by Breakout (3 states, 3 actions).\n",
        "\n",
        "-Implement Monte Carlo (First-visit & Every-visit) evaluation.\n",
        "\n",
        "-Implement Temporal Difference learning (TD(0)).\n",
        "\n",
        "-Apply Value Iteration and Policy Iteration.\n",
        "\n",
        "-Perform Monte Carlo Control with ε-greedy strategy.\n",
        "\n",
        "-Print and compare value functions and policies."
      ],
      "metadata": {
        "id": "Jp02BycXOp1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "SxhSkmiWO1Aq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xp4N_fN3MzwK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defineing Simplified MDP"
      ],
      "metadata": {
        "id": "8Ul1E0tKO5GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states = ['s0', 's1', 's2']  # s2 = terminal\n",
        "actions = ['A1', 'A2', 'A3']\n",
        "\n",
        "# Transition model: {state: {action: (next_state, reward)}}\n",
        "P = {\n",
        "    's0': {'A1': ('s1', 1), 'A2': ('s1', 5), 'A3': ('s2', 0)},\n",
        "    's1': {'A1': ('s2', 0), 'A2': ('s0', 1), 'A3': ('s2', 5)},\n",
        "    's2': {}\n",
        "}\n"
      ],
      "metadata": {
        "id": "Mg4w3EytO3NR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Episodes for Monte Carlo"
      ],
      "metadata": {
        "id": "kgyFrr8VPoNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_episode(policy, start_state='s0'):\n",
        "    episode = []\n",
        "    state = start_state\n",
        "    while state != 's2':\n",
        "        action = policy[state]\n",
        "        next_state, reward = P[state][action]\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    return episode\n",
        "\n",
        "# Random policy for initial exploration\n",
        "random_policy = {\n",
        "    's0': random.choice(actions),\n",
        "    's1': random.choice(actions)\n",
        "}\n"
      ],
      "metadata": {
        "id": "zczcSlPMPppR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monte Carlo First-Visit and Every-Visit"
      ],
      "metadata": {
        "id": "nE1t6ruAPrma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo(policy, first_visit=True, episodes=1000):\n",
        "    returns = defaultdict(list)\n",
        "    V = defaultdict(float)\n",
        "    for _ in range(episodes):\n",
        "        episode = generate_episode(policy)\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s, _, r = episode[t]\n",
        "            G = r + G\n",
        "            if first_visit and s in visited:\n",
        "                continue\n",
        "            visited.add(s)\n",
        "            returns[s].append(G)\n",
        "            V[s] = np.mean(returns[s])\n",
        "    return dict(V)\n",
        "\n",
        "V_mc_first = monte_carlo(random_policy, first_visit=True)\n",
        "V_mc_every = monte_carlo(random_policy, first_visit=False)\n",
        "print(\"V (MC first-visit):\", V_mc_first)\n",
        "print(\"V (MC every-visit):\", V_mc_every)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CUaDh3YPs6K",
        "outputId": "5f6f40fd-33ef-460e-e2f4-7cccdb17e4e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V (MC first-visit): {'s1': np.float64(5.0), 's0': np.float64(6.0)}\n",
            "V (MC every-visit): {'s1': np.float64(5.0), 's0': np.float64(6.0)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TD(0) Learning"
      ],
      "metadata": {
        "id": "GfMx0X2dPvxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def td_0(policy, alpha=0.1, gamma=0.9, episodes=1000):\n",
        "    V = defaultdict(float)\n",
        "    for _ in range(episodes):\n",
        "        state = 's0'\n",
        "        while state != 's2':\n",
        "            action = policy[state]\n",
        "            next_state, reward = P[state][action]\n",
        "            V[state] += alpha * (reward + gamma * V[next_state] - V[state])\n",
        "            state = next_state\n",
        "    return dict(V)\n",
        "\n",
        "V_td = td_0(random_policy)\n",
        "print(\"V (TD(0)):\", V_td)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fh0APAAjPxTK",
        "outputId": "2809cc44-e0e0-452b-c9b8-36dc131b13c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V (TD(0)): {'s0': 5.499999999999993, 's1': 4.9999999999999964, 's2': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value Iteration"
      ],
      "metadata": {
        "id": "4k9QuLaqPzQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(gamma=0.9, theta=1e-3):\n",
        "    V = {s: 0 for s in states}\n",
        "    policy = {}\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in ['s0', 's1']:\n",
        "            best_value = float('-inf')\n",
        "            best_action = None\n",
        "            for a in actions:\n",
        "                if a in P[s]:\n",
        "                    ns, r = P[s][a]\n",
        "                    v = r + gamma * V[ns]\n",
        "                    if v > best_value:\n",
        "                        best_value = v\n",
        "                        best_action = a\n",
        "            delta = max(delta, abs(V[s] - best_value))\n",
        "            V[s] = best_value\n",
        "            policy[s] = best_action\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V, policy\n",
        "\n",
        "V_vi, pi_vi = value_iteration()\n",
        "print(\"Value Iteration V:\", V_vi, \"policy:\", pi_vi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsKdvU-pP1nq",
        "outputId": "0a278c3f-8ffc-43d6-f56b-8ddf2de08633"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Iteration V: {'s0': 31.0488971655632, 's1': 28.944007449006882, 's2': 0} policy: {'s0': 'A2', 's1': 'A2'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Policy Iteration"
      ],
      "metadata": {
        "id": "cE6Ov0etP8gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(gamma=0.9, theta=1e-3):\n",
        "    policy = {s: random.choice(actions) for s in ['s0', 's1']}\n",
        "    V = {s: 0 for s in states}\n",
        "    is_stable = False\n",
        "\n",
        "    while not is_stable:\n",
        "        # Policy Evaluation\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in ['s0', 's1']:\n",
        "                ns, r = P[s][policy[s]]\n",
        "                v = r + gamma * V[ns]\n",
        "                delta = max(delta, abs(V[s] - v))\n",
        "                V[s] = v\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "        # Policy Improvement\n",
        "        is_stable = True\n",
        "        for s in ['s0', 's1']:\n",
        "            old_action = policy[s]\n",
        "            best_action = max(actions, key=lambda a: P[s][a][1] + gamma * V[P[s][a][0]])\n",
        "            policy[s] = best_action\n",
        "            if old_action != best_action:\n",
        "                is_stable = False\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "V_pi, pi_pi = policy_iteration()\n",
        "print(\"Policy Iteration V:\", V_pi, \"policy:\", pi_pi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-ANppa9QIiC",
        "outputId": "230fe3bd-557f-4dc6-c30b-3ff41444b046"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Iteration V: {'s0': 31.048817531607753, 's1': 28.943935778446978, 's2': 0} policy: {'s0': 'A2', 's1': 'A2'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monte Carlo Control (ε-greedy)"
      ],
      "metadata": {
        "id": "cpC85cvlQLUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mc_control(epsilon=0.1, gamma=0.9, episodes=5000):\n",
        "    Q = defaultdict(lambda: {a: 0 for a in actions})\n",
        "    returns = defaultdict(lambda: {a: [] for a in actions})\n",
        "    policy = {s: random.choice(actions) for s in ['s0', 's1']}\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        # ε-greedy policy\n",
        "        def select_action(s):\n",
        "            if random.random() < epsilon:\n",
        "                return random.choice(actions)\n",
        "            return max(Q[s], key=Q[s].get)\n",
        "\n",
        "        episode = []\n",
        "        state = 's0'\n",
        "        while state != 's2':\n",
        "            action = select_action(state)\n",
        "            next_state, reward = P[state][action]\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for t in reversed(range(len(episode))):\n",
        "            s, a, r = episode[t]\n",
        "            G = r + gamma * G\n",
        "            if (s, a) not in visited:\n",
        "                visited.add((s, a))\n",
        "                returns[s][a].append(G)\n",
        "                Q[s][a] = np.mean(returns[s][a])\n",
        "                policy[s] = max(Q[s], key=Q[s].get)\n",
        "\n",
        "    return policy, Q\n",
        "\n",
        "policy_mc, Q_mc = mc_control()\n",
        "print(\"MC Control found policy:\", policy_mc)\n",
        "print(\"Q(s0):\", Q_mc['s0'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3CbOYSiQMna",
        "outputId": "ac5eb680-786a-4a57-bfde-f801da6894da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MC Control found policy: {'s0': 'A2', 's1': 'A3'}\n",
            "Q(s0): {'A1': np.float64(8.661818637346379), 'A2': np.float64(8.66869327500488), 'A3': np.float64(0.0)}\n"
          ]
        }
      ]
    }
  ]
}